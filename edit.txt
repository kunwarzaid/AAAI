\section{Introduction}

Large Language Models (LLMs) such as GPT-4, Gemini, and Med-PaLM 2 have achieved strong performance on medical question-answering tasks, reaching near-clinician accuracy on benchmarks like MedQA. These results suggest potential use in decision support and triage, but reliability remains a key obstacle. The same open-ended reasoning that gives LLMs flexibility also makes their outputs difficult to verify in safety-critical settings such as clinical care.

In medicine, accuracy alone is insufficient. Clinicians expect models to justify conclusions with clear reasoning, evidence, and awareness of uncertainty. Without such transparency, correct predictions can still be unsafe if they arise from flawed or unverifiable logic. Prior studies note that LLMs often express unwarranted confidence, provide inconsistent explanations, or change answers with small prompt variations—symptoms of weak reasoning stability and poor calibration.

Earlier work, including Med-Guard, focused mainly on safety supervision. That system reduced unsafe prescriptions and redundant tests by adding a safety agent but still offered limited visibility into how diagnostic conclusions were reached. The reasoning process itself remained a black box.

This study introduces \textbf{Trust-X}, a multi-agent setup designed to evaluate and quantify trust in model-based clinical reasoning. Rather than asking only whether outcomes are safe, Trust-X examines \emph{how} models reach decisions. It does so through four key components:
\begin{itemize}
    \item \textbf{Consensus reasoning:} multiple doctor agents independently assess each case; their disagreement, measured by the \textit{Consensus Disagreement Rate (CDR)}, reflects epistemic uncertainty.
    \item \textbf{Structured reasoning traces:} each diagnostic step includes a \texttt{<thinking\_process>} record linking evidence and conclusions.
    \item \textbf{Logging and traceability:} all interactions are timestamped and stored for causal replay and audit.
    \item \textbf{Trust metrics:} composite indices—Epistemic (ETI), Operational (OSI), and Final (FTI)—combine reasoning coherence and safety behavior into interpretable scores.
\end{itemize}

Trust-X does not claim to make models inherently trustworthy; rather, it exposes where reasoning is consistent, cautious, or prone to error. This approach treats trust as a measurable property of reasoning behavior, not just predictive accuracy.

\section{Related Work}

Explainability research has long sought to make model predictions interpretable. Early work used feature attribution and surrogate models—such as LIME, SHAP, and Grad-CAM—to highlight influential inputs. These approaches work for static classifiers but not for conversational or reasoning-based models, where decisions unfold over multiple steps. In medicine, explainability further requires \emph{causability}: a traceable link between evidence, inference, and outcome.

Recent advances in LLMs have shifted focus to reasoning traces. Methods like chain-of-thought prompting, self-consistency, and reflective reasoning externalize a model’s intermediate steps. However, such traces often describe reasoning after the fact rather than during it, and may appear coherent even when conclusions are wrong. Transparency therefore does not guarantee truthful reasoning.

Within medical AI, explainability and safety are closely linked. Clinicians assess models not only by accuracy but by the soundness of their reasoning. While medical LLMs such as BioGPT, Med-PaLM 2, and ChatDoctor achieve strong factual accuracy, their diagnostic reasoning remains opaque. Systems like Clinical-Camel introduced dialogue-based reasoning but still operate as single agents, without consensus or uncertainty estimation.

Multi-agent systems have begun to address these gaps. AgentClinic modeled doctor–patient interactions using reasoning and data agents to improve conversational realism. Safety-oriented frameworks such as GuardMed and SafetyBench added oversight mechanisms but focused mainly on moderating outputs rather than explaining reasoning. Regulatory guidelines from the World Health Organization and the European Commission emphasize traceability and auditability as prerequisites for trustworthy AI, yet most current pipelines still lack such transparency during inference.

\textbf{Trust-X} builds on these efforts by integrating consensus reasoning, structured traces, and continuous logging into one process. Every agent interaction is recorded and linked to contextual evidence, allowing inspection of how conclusions evolve. This design provides \emph{process-level transparency}, enabling verification of both reasoning quality and safety behavior within complex diagnostic tasks.
