\section{Methodology: The Trust-X Framework}

Building upon the safety-centered architecture, \textbf{Trust-X} re-conceptualizes medical diagnostic reasoning as a transparent, auditable, and multi-agent process. Its goal extends beyond producing \textit{safe} clinical outputs to ensuring that reasoning itself is \textit{trustworthy}—explainable, consistent, and accountable.

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{Doctor Agent (1).png}
\caption{
System architecture of \textbf{Trust-X}. Multiple Doctor Agents perform independent reasoning over patient data and measurements under a Consensus Module that quantifies diagnostic disagreement (CDR). Safety agents provide real-time oversight of tests and prescriptions, while an Explainability and Logging Layer records reasoning traces, timestamps, and safety flags. Logged evidence is analyzed by the Trust Metric Engine to compute epistemic (ETI), operational (OSI), and final (FTI) trust indices—linking reasoning transparency with quantifiable reliability.
}
\label{fig:logging-architecture}
\end{figure*}

This design directly addresses two persistent challenges in clinical AI: (i) the \textit{lack of visibility into intermediate reasoning steps}, and (ii) the \textit{difficulty of quantifying uncertainty and trustworthiness}. Trust-X resolves both through structured multi-agent interaction and a suite of multi-dimensional trust metrics.

\subsection{Multi-Agent Diagnostic Simulation}

Trust-X models realistic clinical encounters as iterative dialogues among specialized agents:

\begin{itemize}
    \item \textbf{Doctor Agent:} Conducts hypothesis-driven reasoning, asks questions, orders investigations, and outputs structured conclusions of the form \texttt{DIAGNOSIS READY: <diagnosis>}.
    \item \textbf{Patient Agent:} Retrieves structured case information and provides consistent, factual responses grounded in the case file.
    \item \textbf{Measurement Agent:} Simulates diagnostic tests, returning results or indicating unavailable data.
    \item \textbf{Safety Agent:} Monitors proposed actions (tests and prescriptions) for risks or contraindications.
\end{itemize}

Each consultation proceeds in rounds of doctor–patient exchanges until a stable diagnosis and treatment plan emerge. This decomposition enables independent auditing of reasoning quality, safety behavior, and epistemic stability—critical for understanding how LLMs behave under clinical uncertainty.

\subsection{Consensus Reasoning and Epistemic Uncertainty}

To model epistemic uncertainty—\textit{what the system does not know that it does not know}—Trust-X employs \textbf{consensus reasoning}. 
Instead of relying on a single deterministic Doctor Agent, $K$ independent replicas analyze the same case in parallel. 
Their individual diagnoses $\{d_{i1}, \dots, d_{iK}\}$ are aggregated via majority voting, and their level of disagreement defines the \textbf{Consensus Disagreement Rate (CDR)}:
\[
\mathrm{CDR}_i = 1 - \frac{\max_{d \in D_i} f_i(d)}{K},
\]
where $f_i(d)$ is the frequency of diagnosis $d$ among the $K$ agents for case $i$. All experiments used $K=3$ independent Doctor Agents; tie-breaks defaulted to the lexicographically earliest diagnosis label.

This formulation follows standard inter-rater disagreement measures and quantifies epistemic uncertainty as the fraction of non-consensus across agents.  
High CDR values indicate divergent diagnostic reasoning, while low CDR reflects convergence and epistemic stability.  

\subsection{Reasoning Traces and the Differential Diagnosis Lifecycle}

Each Doctor Agent produces structured reasoning encapsulated in \texttt{<thinking\_process>} tags, including:
\begin{itemize}
    \item intermediate hypotheses and their supporting evidence,
    \item motivations for diagnostic test orders, and
    \item discarded hypotheses with rationales.
\end{itemize}

Over successive rounds, the system constructs a \textbf{reasoning ledger}—a sequence of hypothesis updates forming the \textbf{Differential Diagnosis Lifecycle (DDxL)}. 
This mirrors clinical reasoning, where hypotheses are refined as new evidence emerges, enabling quantitative analysis of reasoning patterns such as redundancy, coverage, and internal coherence.
