\subsection{System Variants}
Four configurations were evaluated:
\begin{itemize}
    \item \textbf{Baseline:} Doctor–Patient–Measurement only.
    \item \textbf{Safety:} adds safety supervision, consensus off.
    \item \textbf{Consensus:} adds multi-doctor reasoning, safety off.
    \item \textbf{Trust (Full):} all agents active.
\end{itemize}

\section{Results and Discussion}
\label{sec:results}

\subsection{Quantitative Outcomes}
Accuracy remained stable across configurations (68–69\%), indicating that variations in trust metrics originate from reasoning design and safety activation, not from underlying model capacity.  
Under the revised Operational Safety Index (OSI), systems without active safety agents (\textbf{Baseline}, \textbf{Consensus}) now receive OSI = 0, leading to proportionally lower Final Trust Index (FTI) values despite high accuracy.  
This correction removes the artificial inflation observed in earlier analyses and ensures that operational trust can only be achieved when safety supervision is actually active.
Trust indices were computed according to Algorithm \ref{alg:trustmetrics}, ensuring consistency across configurations

In contrast, \textbf{Safety} and \textbf{Trust} configurations—both employing real-time safety agents—achieved substantially higher FTI (75.4), demonstrating that transparent oversight and risk-aware reasoning are the true drivers of trust, not performance alone. Values reflect mean performance across 50 cases; standard errors averaged ±1.8\%.


\subsection{Trust–Accuracy Relationship}
Figure~\ref{fig:bubble_tradeoff} illustrates the trust–accuracy trade-off.  
All systems perform similarly in diagnostic accuracy, yet their trustworthiness diverges sharply.  
Baseline and Consensus occupy the lower region of the trust axis FTI (35.2–39.2), reflecting the absence of active safety reasoning.  
Safety and Trust, on the other hand, attain higher FTI scores (75.4) by explicitly detecting and mitigating unsafe actions.  
The result underscores that \emph{trustworthiness must be demonstrated through reasoning behavior, not inferred from accuracy metrics}.

\subsection{Reasoning Quality Metrics}

Figure~\ref{fig:reasoning_quality} compares reasoning-trace features.  
\textbf{Safety} and \textbf{Trust} modes produced the longest reasoning chains (≈ 300–350 tokens), a result of explicit verification and safety deliberation steps.  
\textbf{Consensus} reasoning—though shorter—achieved the highest coherence (RDC = 73.8) and evidence coverage, but also higher redundancy due to repeated cross-agent discussions.  
This indicates that consensus mechanisms foster reflective reasoning and self-agreement, while safety supervision enhances thoroughness even at the cost of brevity.

\subsection{Safety and Explainability}

Figure~\ref{fig:safety_explainability} depicts how safety oversight influences reasoning coherence.  
\textbf{Safety} and \textbf{Trust} configurations generated numerous safety alerts and DDI detections, confirming the vigilance of the safety agents.  
While Safety mode reduced coherence (RDC ≈ 65) due to frequent interruptions, the \textbf{Trust} mode recovered much of this stability by incorporating consensus reasoning, blending caution with interpretability.  
This demonstrates that structured safety reasoning—when integrated with deliberative multi-agent exchange—can enhance both prudence and clarity.

\subsection{Multidimensional Trust Profile}

Figure~\ref{fig:trust_radar} visualizes trust dimensions including accuracy, coherence, evidence coverage, and safety proxies (inverted UnsafeRx and TestAlerts).  
Baseline and Consensus exhibit narrow, safety-deficient profiles, while Safety and Trust show broader, balanced patterns.  
The Trust configuration provides the most symmetrical radar footprint, combining coherent reasoning, active safety checks, and strong evidence grounding.  
This multidimensional balance reinforces the design goal of Trust-X: \emph{trust as an emergent property of transparent, reasoned supervision}.

\subsection{Qualitative Reasoning Behavior}

Qualitative review of 50 reasoning traces confirmed four distinct behavioral archetypes:
\begin{itemize}
    \item \textbf{Baseline:} fluent but overconfident reasoning with no explicit safety awareness.
    \item \textbf{Safety:} self-regulating and cautious, often interrupted by safety rejections.
    \item \textbf{Consensus:} deliberative, uncertainty-aware reasoning across multiple agents.
    \item \textbf{Trust:} human-like reflective dialogue that unifies verification and consensus.
\end{itemize}
These behaviors parallel the quantitative patterns—Trust integrates both epistemic and operational reliability within a single reasoning cycle.





\subsection{Discussion}

The revised metrics and results resolve the key reviewer concerns on metric confounding and narrative alignment.  
By enforcing OSI = 0 when safety supervision is inactive, we eliminate prior over-scoring of unsupervised systems and ensure that high trust values correspond to genuine oversight.  
\textbf{Baseline} and \textbf{Consensus} now reflect epistemic reliability without operational safety, while \textbf{Safety} and \textbf{Trust} represent end-to-end responsible reasoning.

Overall, Trust-X demonstrates that medical LLMs achieve trustworthy performance only when accuracy, explainability, and safety reasoning co-evolve.  
Safety mechanisms lower apparent performance but elevate interpretive accountability—aligning quantitative indices with qualitative behavior.  
This result strengthens the core claim: \emph{trust cannot be inherited from accuracy—it must be earned through transparent reasoning.}

\section{Conclusion}
\label{sec:conclusion}

Large language models may display competent diagnostic reasoning yet remain untrustworthy under clinical scrutiny.  
By embedding explainability, consensus, and safety into its reasoning pipeline, Trust-X transforms correctness into accountability.  
The revised OSI formulation ensures that systems cannot appear trustworthy without genuine supervision, reframing evaluation from ``Was the answer correct?'' to ``Was the reasoning responsible?''  
Trust-X thus establishes a foundation for trust-centered explainability, where safety reasoning and epistemic coherence jointly define reliability.We acknowledge that our evaluation, based on 50 cases per configuration (200 total simulations), limits statistical generalization. 
However, the study’s intent was diagnostic—probing whether LLMs exhibit trustworthy reasoning under controlled conditions—rather than producing a large-scale benchmark. 
The observed patterns were consistent across all four settings, suggesting that the core trust behaviors are robust even in a limited sample.





\section{Ethical and Regulatory Considerations}
Trust-X is designed for research use only and does not provide clinical decisions.
All experiments were conducted on synthetic or public benchmark data.
The framework supports traceability, safety logging, and human-in-the-loop review, aligning with FDA and EMA guidance on AI transparency.
