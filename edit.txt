\section{Introduction}

Large Language Models (LLMs) such as GPT-4, Gemini, and Med-PaLM 2 have shown strong results in tasks across many fields, including medicine \cite{kung2023performance,nori2023capabilities,lee2023benefits}. They perform close to clinician levels on medical question-answering tasks, suggesting possible use in diagnosis, documentation, and triage. However, their fluency and broad knowledge also make them unreliable when accuracy and reasoning are critical \cite{ji2023survey,begoli2019need}.

In medicine, accuracy alone is not enough. Clinicians need to understand \textit{why} a decision was made. Medical reasoning involves understanding cause and effect, managing uncertainty, and weighing evidence—skills that current models only partly show. When an AI suggests a diagnosis, trust depends not only on the answer but also on the reasoning behind it \cite{tonekaboni2019clinicians,amann2020explainability}. Without clear reasoning, even a correct answer may be unsafe.

While alignment and safety testing have improved model behavior \cite{mei2023assert}, trust is still a major issue. Clinical LLMs often show unjustified confidence, inconsistent logic, and poor handling of uncertainty \cite{a2019,huang2025survey,kadavath2022language,guo2017calibration,malinin2020uncertainty}. In healthcare, admitting uncertainty is often safer than giving a wrong but confident answer. Systems like Med-PaLM 2, BioGPT, and ChatDoctor can be accurate but unstable—the same case may lead to different diagnoses with small prompt changes \cite{luo2022biogpt,li2023chatdoctor}.

Med-Guard \cite{jain2025medguard} improved safety by using four specialized agents—Doctor, Patient, Measurement, and Safety—to simulate a clinical process. It reduced unsafe test orders and prescriptions through structured dialogue and explicit safety checks. However, its reasoning was still hidden, and clinicians could not trace how or why conclusions were reached.

These issues led to \textbf{Trust-X}, a system focused on \textit{trust through explainability}.  
While Med-Guard asked “Is the outcome safe?”, Trust-X asks “Can we trust how the model reached it?”  

Trust-X includes four main features:
\begin{itemize}
    \item \textbf{Consensus Reasoning:} Several Doctor Agents analyze each case independently and vote on the best diagnosis. Their disagreement, measured by the \textit{Consensus Disagreement Rate (CDR)}, shows uncertainty.
    \item \textbf{Reasoning Trace:} Each diagnostic step includes a clear trace linking evidence, reasoning, and results, showing how thoughts evolve.
    \item \textbf{End-to-End Logging:} Every agent action is recorded with time, role, and reason, allowing full replay and accountability.
    \item \textbf{Trust Scoring:} A trust layer combines interpretability, safety, and reasoning quality into measurable trust scores.
\end{itemize}

Together, these parts make reasoning visible and testable. Logging turns black-box behavior into traceable steps; consensus captures uncertainty; and trust scores connect reasoning quality with safety. Trust-X does not claim to make models fully trustworthy—it helps reveal where trust is earned and where it breaks down.

\section{Related Work}

Explainability research in AI has long tried to make model decisions easier to understand. Early Explainable AI (XAI) tools such as LIME \cite{ribeiro2016should}, SHAP \cite{lundberg2017unified}, and Grad-CAM \cite{selvaraju2017grad} showed which features influenced predictions. These work well for static models but not for medicine, where decisions evolve through dialogue and evidence. In healthcare, true explainability means linking evidence, reasoning, and outcome in a way humans can follow \cite{holzinger2019causability,ahmad2018interpretable}.

LLMs brought new ways to show reasoning through text. Methods like chain-of-thought prompting \cite{wei2022chain}, self-consistency \cite{wang2022self}, and reflection \cite{shinn2023reflexion} make models write their reasoning. However, these traces are often just post-hoc explanations, not real reasoning \cite{turpin2023language}. Models may sound confident but still be wrong \cite{kadavath2022language,ji2023survey}.

In medicine, explainability and safety are closely linked. Clinicians judge AI systems by how well their reasoning follows medical logic \cite{tonekaboni2019clinicians,amann2020explainability}. LLM-based systems such as BioGPT \cite{luo2022biogpt}, Med-PaLM 2 \cite{singhal2025toward}, and ChatDoctor \cite{li2023chatdoctor} perform well factually but remain opaque. Clinical-Camel \cite{toma2023clinical} added interactive dialogue but still lacked uncertainty tracking.

Multi-agent frameworks are starting to address these problems. AgentClinic \cite{schmidgall2024agentclinic} modeled doctor–patient conversation between reasoning and data agents, improving dialogue quality but not full accountability. Other systems, such as GuardMed and SafetyBench, focus on safety moderation but not on making reasoning transparent.

Regulatory bodies like the World Health Organization \cite{guidance2021ethics} and the European Commission \cite{bomhard2021regulation} highlight traceability and auditability as key for trustworthy medical AI. Still, most LLM systems remain black boxes during inference, offering little insight into their internal reasoning \cite{begoli2019need,doshi2017towards}.

\textbf{Trust-X} builds on these efforts by making explainability part of the reasoning process. It records reasoning steps, agent interactions, and consensus decisions in real time. Each exchange is logged with context, creating a full trail that clinicians can review. Unlike post-hoc methods, Trust-X captures how evidence leads to conclusions, helping clinicians inspect, question, and verify the model’s thought process.
