\section{Introduction}

Large Language Models (LLMs) like GPT-4, Gemini, and Med-PaLM 2 have shown strong performance in many fields, including medicine \cite{kung2023performance,nori2023capabilities,lee2023benefits}. They can answer medical questions at near-clinician levels, suggesting they could one day help with diagnosis, documentation, or triage. However, this fluency can also be a weakness. Models can be unreliable in situations where precise reasoning is essential.

In medicine, accuracy alone is not enough. Clinicians must also understand \textit{why} a model made a specific decision. Medical reasoning is complex; it involves finding cause and effect, managing uncertainty, and weighing evidence. These are skills that current models only partially possess. When an AI suggests a diagnosis, a clinician's trust depends on seeing the reasoning behind it. Without a clear explanation, even a correct answer may be unsafe to use.

While safety testing has improved model behavior \cite{mei2023assert}, building trust remains a major challenge. Clinical LLMs often show unjustified confidence, apply logic inconsistently, and fail to handle uncertainty well \cite{a2019,huang2025survey,guo2017calibration,malinin2020uncertainty}. In healthcare, admitting "I don't know" is often safer than giving a wrong but confident answer. Furthermore, many models can be unstable. Giving the same case file to a model twice, with only small changes to the prompt, can lead to two different diagnoses.

One system, Med-Guard, improved safety by using specialized agents (Doctor, Patient, etc.) to simulate a clinical process \cite{jain2025medguard}. This method reduced unsafe test orders and prescriptions by using structured dialogue and safety checks. However, its reasoning remained a "black box," leaving clinicians unable to trace how or why a conclusion was reached.

These challenges led us to develop \textbf{Trust-X}, a system designed to build trust by making the model's reasoning process clear.
Our approach shifts the question from Med-Guard’s “Is the outcome safe?” to “Can we trust \textit{how} the model reached this outcome?”

Trust-X has four main features:
\begin{itemize}
    \item \textbf{Consensus Reasoning:} Instead of one, several "Doctor" agents analyze a case. They vote on the diagnosis, and their level of disagreement is used to measure the system's uncertainty.
    \item \textbf{Reasoning Trace:} Each step of the diagnosis includes a clear trace that links the evidence (like symptoms) to the reasoning and then to the final result.
    \item \textbf{Complete Logging:} Every action taken by every agent is recorded, including the time, the agent's role, and its stated reason for the action. This allows for a full review of the process.
    \item \textbf{Trust Scoring:} A new "trust layer" measures the quality of the reasoning, its interpretability, and its safety, combining them into a final trust score.
\end{itemize}

Together, these features make the model's reasoning visible and testable. Logging turns "black-box" behavior into a series of traceable steps. The consensus process clearly shows uncertainty. The trust scores connect the quality of the reasoning to patient safety. Trust-X does not claim to solve trust; rather, it provides tools to show where trust is earned and where it breaks down.

\section{Related Work}

Explainability research in AI has long tried to make model decisions easier to understand. Early Explainable AI (XAI) tools such as LIME \cite{ribeiro2016should}, SHAP \cite{lundberg2017unified}, and Grad-CAM \cite{selvaraju2017grad} showed which features influenced predictions. These work well for static models but not for medicine, where decisions evolve through dialogue and evidence. In healthcare, true explainability means linking evidence, reasoning, and outcome in a way humans can follow \cite{holzinger2019causability,ahmad2018interpretable}.

LLMs brought new ways to show reasoning through text. Methods like chain-of-thought prompting \cite{wei2022chain}, self-consistency \cite{wang2022self}, and reflection \cite{shinn2023reflexion} make models write their reasoning. However, these traces are often just post-hoc explanations, not real reasoning \cite{turpin2023language}. Models may sound confident but still be wrong \cite{kadavath2022language,ji2023survey}.

In medicine, explainability and safety are closely linked. Clinicians judge AI systems by how well their reasoning follows medical logic \cite{tonekaboni2019clinicians,amann2020explainability}. LLM-based systems such as BioGPT \cite{luo2022biogpt}, Med-PaLM 2 \cite{singhal2025toward}, and ChatDoctor \cite{li2023chatdoctor} perform well factually but remain opaque. Clinical-Camel \cite{toma2023clinical} added interactive dialogue but still lacked uncertainty tracking.

Multi-agent frameworks are starting to address these problems. AgentClinic \cite{schmidgall2024agentclinic} modeled doctor–patient conversation between reasoning and data agents, improving dialogue quality but not full accountability. Other systems, such as GuardMed and SafetyBench, focus on safety moderation but not on making reasoning transparent.

Regulatory bodies like the World Health Organization \cite{guidance2021ethics} and the European Commission \cite{bomhard2021regulation} highlight traceability and auditability as key for trustworthy medical AI. Still, most LLM systems remain black boxes during inference, offering little insight into their internal reasoning \cite{begoli2019need,doshi2017towards}.

\textbf{Trust-X} builds on these efforts by making explainability part of the reasoning process. It records reasoning steps, agent interactions, and consensus decisions in real time. Each exchange is logged with context, creating a full trail that clinicians can review. Unlike post-hoc methods, Trust-X captures how evidence leads to conclusions, helping clinicians inspect, question, and verify the model’s thought process.
