\subsection{Safety and Prescription Oversight}

Unlike systems that apply safety filters post hoc, Trust-X enforces \textit{real-time safety feedback} through two dedicated agents:

\begin{itemize}
    \item \textbf{Test Safety Agent:} Screens diagnostic test orders before execution, flagging redundant or high-risk investigations (e.g., unnecessary imaging).
    \item \textbf{Prescription Safety Agent:} Reviews generated prescriptions for contraindications and drug–drug interactions (DDIs), labeling them as \textit{SAFE}, \textit{CAUTION}, or \textit{UNSAFE}.
\end{itemize}

Safety thus acts as an active constraint that shapes the reasoning trajectory rather than a passive post-hoc evaluator. 
The frequency of alerts defines the \textit{Test Safety Alert Rate} and \textit{Unsafe Prescription Percentage} metrics used in evaluation.
Safety labels were determined using DrugBank \cite{wishart2018drugbank} for DDI reference and an LLM-based safety prompt for clinical context checks.
The latter acted only as a classifier, not as a reasoning agent, to avoid self-assessment bias.


\subsection{Explainability and Logging Layer}

All inter-agent communications—questions, test requests, safety warnings, and reasoning updates—are captured by a unified \textbf{Explainability and Logging Layer}. 
Each record includes:
\begin{enumerate}
    \item the acting agent and its role,
    \item timestamp and case context,
    \item the current reasoning trace, and
    \item trust-relevant metadata (confidence, CDR, safety flags).
\end{enumerate}
This continuous, machine-readable log enables causal replay, bias audits, and trust analysis, forming the audit trail required for verifiable explainability under clinical AI standards.

\subsection{Trust Metrics: From Transparency to Quantification}

Beyond safety alone, a clinically trustworthy system must reason coherently, act prudently, and remain stable across similar cases. 
Trust-X formalizes these through three indices: the \textbf{Epistemic Trust Index (ETI)}, the \textbf{Operational Safety Index (OSI)}, and their composite \textbf{Final Trust Index (FTI)}.

\paragraph{Epistemic Trust Index (ETI).}
ETI quantifies reasoning reliability by combining diagnostic accuracy, inter-agent stability, and reasoning consistency:
\[
\mathrm{ETI} = 0.4 \times \mathrm{Accuracy} + 0.3 \times (1 - \mathrm{CDR}) + 0.3 \times \mathrm{RDC}.
\]
Weights $(0.4, 0.3, 0.3)$ reflect clinical reasoning priorities—correctness first, but stability and justification nearly as important—and were verified robust via ablation (Table~\ref{tab:eti_weight_sweep}). 

\paragraph{Reasoning–Diagnosis Consistency (RDC).}
RDC measures semantic alignment between an agent’s reasoning trace and its final diagnosis using cosine similarity between sentence embeddings:
\[
\mathrm{RDC}_i = \cos(E(t_i), E(d_i)),
\]
where $E(\cdot)$ uses the \texttt{SentenceTransformer(all-MiniLM-L6-v2)} encoder \cite{reimers2019sentencebert}.  
RDC captures internal consistency rather than clinical correctness: a model can be self-consistent yet incorrect. 
To calibrate interpretability, we compared RDC to expert-rated coherence in a 10-case subset, observing a moderate correlation ($r{=}0.52$). RDC measures internal textual coherence but does not claim medical validity; it complements, rather than replaces, expert evaluation.

\paragraph{Operational Safety Index (OSI).}
To prevent artificial inflation of safety scores, OSI is defined as zero when no safety agents are active.
Otherwise, it penalizes unsafe prescriptions and test alerts equally:

$\mathrm{OSI}=0$ when safety agents are inactive (Baseline, Consensus),
and $\mathrm{OSI}=100 - 0.5(\text{UnsafeRx\%} + \text{TestAlerts\%})$ otherwise.
This formulation ensures that operational trust cannot be earned without actual safety supervision.


\paragraph{Final Trust Index (FTI).}
FTI integrates epistemic soundness (ETI) and behavioral safety (OSI):
\[
\mathrm{FTI} = 0.5 \times \mathrm{ETI} + 0.5 \times \mathrm{OSI}.
\]
Two variants are reported: $\mathrm{FTI}_{\text{SafetyON}}$ and $\mathrm{FTI}_{\text{SafetyOFF}}$, corresponding to configurations with and without active safety agents. 
This prevents structural inflation of trust scores in safety-disabled settings and allows separation of reasoning transparency from operational vigilance.
